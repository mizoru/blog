{
  
    
        "post0": {
            "title": "Predicting Japanese pitch accent",
            "content": "I made a model that predicts Japanese pitch accent that you can try out here: https://huggingface.co/spaces/mizoru/Japanese_pitch . But what is pitch accent and how did I make this? . Pitch accent . Let&#39;s look at two Japanese words: 飴 &quot;candy&quot; and 雨 &quot;rain&quot;. They&#39;re both spelled ame, but they don&#39;t sound the same. Let&#39;s listen to how they&#39;re pronounced! . Your browser does not support the audio element. Your browser does not support the audio element. Can you tell the difference? The second word has a drop in pitch after the first vowel. . This is pitch accent. In pitch-accent languages one syllable of the word can be marked with contrasting pitch, rather than by loudness or length as in languages with stress accent. Various pitch accent systems exist in many languages, in Swedish, Norwegian, Serbo-Croatian and Lithuanian among others. In fact, pitch accent is reconstructed for Proto-Indo-European, the proto-language of most of the languages of Europe. . Japanese pitch accent . In Japanese pitch accent is caracterized by a drop in pitch. The mora preceding the drop is said to have the accent. So the pitch accent of a word can be described with one number, designating the mora that carries the accent. The word 雨 &quot;rain&quot; has the accent on the first mora, the pitch accent of it would be written down as 「１」. But there&#39;s a different way to classify words by pitch accent. . When the first mora a word carries the accent, the pitch accent pattern of the word is 頭高 atamadaka, literally &quot;head-high&quot;. If the pitch drops after that the pitch accent pattern is 中高 nakadaka, &quot;middle-high&quot;. Many words in Japanese don&#39;t have a drop in pitch, these are called 平板 heiban, &quot;flat&quot; . Let&#39;s listen to some examples. . Atamadaka: . Your browser does not support the audio element. Nakadaka: . Your browser does not support the audio element. Heiban: . Your browser does not support the audio element. In some words, the drop in pitch is apparent only if there is a particle attached to the word. In these words the last mora of the word is accented, so the particle attached after it is low. These words have the pitch accent pattern 尾高 odaka, &quot;tail-high&quot;. In practice, the phonetic word sounds heiban when there&#39;s no particle attached, and sounds nakadaka, when there is. So we&#39;re not going to use these for training. . Data . I thank Yoga from Migaku for making this data available. . Let&#39;s import the libraries we&#39;re going to be using to make the model. . from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * . Here are fastai, fastaudio and torchaudio versions accordingly: . import fastai import fastaudio import torchaudio fastai.__version__, fastaudio.__version__,torchaudio.__version__ . (&#39;2.3.1&#39;, &#39;1.0.2&#39;, &#39;0.8.1&#39;) . Let&#39;s take a look at our data . Path(&quot;dict1&quot;).ls() . (#80120) [Path(&#39;dict1/α粒子.yomi00013233_063E.mp3&#39;),Path(&#39;dict1/α線.yomi0001323E_0162.mp3&#39;),Path(&#39;dict1/γ.yomi00013247_04EE.mp3&#39;),Path(&#39;dict1/γ線.yomi0001324B_0100.mp3&#39;),Path(&#39;dict1/λ.yomi00013255_034C.mp3&#39;),Path(&#39;dict1/π.yomi0001325F_0238.mp3&#39;),Path(&#39;dict1/σ.yomi00013265_07D4.mp3&#39;),Path(&#39;dict1/ω.yomi0001326D_05E0.mp3&#39;),Path(&#39;dict1/○×式.yomi00013273_004A.mp3&#39;),Path(&#39;dict1/ああ.yomi00013280_030E.mp3&#39;)...] . Path(&quot;dict2&quot;).ls() . (#84356) [Path(&#39;dict2/あくどい-2830_1_1_male.mp3&#39;),Path(&#39;dict2/あくどい-2830_2_1_female.mp3&#39;),Path(&#39;dict2/あくどい-2830_2_1_male.mp3&#39;),Path(&#39;dict2/あくどいです-2830_3_1_female.mp3&#39;),Path(&#39;dict2/あくどいです-2830_3_1_male.mp3&#39;),Path(&#39;dict2/あくどかった-2830_6_1_female.mp3&#39;),Path(&#39;dict2/あくどかった-2830_6_1_male.mp3&#39;),Path(&#39;dict2/あくどかった-2830_6_2_female.mp3&#39;),Path(&#39;dict2/あくどかった-2830_6_2_male.mp3&#39;),Path(&#39;dict2/あくどく-2830_4_1_female.mp3&#39;)...] . dict1 = pd.read_csv(&#39;dict1_labels.csv&#39;) . dict1 . path pattern kana morae drop type . 0 dict1/ある.yomi000142BB_0596.mp3 | 頭高 | アル | 2 | 1 | dict1 | . 1 dict1/思う.yomi0006C617_043A.mp3 | 中高 | オモウ | 3 | 2 | dict1 | . 2 dict1/など.yomi000240B7_0028.mp3 | 頭高 | ナド | 2 | 1 | dict1 | . 3 dict1/私.yomi00092F63_0072.mp3 | 平板 | ワタくシ | 4 | 0 | dict1 | . 4 dict1/見る.yomi000A41BD_001E.mp3 | 頭高 | ミル | 2 | 1 | dict1 | . ... ... | ... | ... | ... | ... | ... | . 79480 dict1/捨てがな_捨て仮名.yomi00072538_06BE.mp3 | 平板 | すテカ゚ナ | 5 | 0 | dict1 | . 79481 dict1/くも膜下出血_蜘蛛膜下出血.yomi0001AAD1_0622.mp3 | 中高 | クモマッカしュッケツ | 9 | 6 | dict1 | . 79482 dict1/捜す.yomi00072507_0088.mp3 | 平板 | サカ゚ス | 4 | 0 | dict1 | . 79483 dict1/捜し物.yomi000724FD_0424.mp3 | 平板 | サカ゚シモノ | 6 | 0 | dict1 | . 79484 dict1/あこや貝_阿古屋貝.yomi00013767_0114.mp3 | 中高 | アコヤカ゚イ | 6 | 3 | dict1 | . 76293 rows × 6 columns . dict2 = pd.read_csv(&#39;dict2_labels.csv&#39;) . dict2 . path pattern kana morae drop type . 0 dict2/ある-66_1_1_male.mp3 | 頭高 | ある | 2 | 1 | dict2 male | . 1 dict2/ある-66_1_1_female.mp3 | 頭高 | ある | 2 | 1 | dict2 female | . 2 dict2/あります-66_2_1_male.mp3 | 中高 | あります | 4 | 3 | dict2 male | . 3 dict2/あります-66_2_1_female.mp3 | 中高 | あります | 4 | 3 | dict2 female | . 4 dict2/あって-66_3_1_male.mp3 | 頭高 | あって | 3 | 1 | dict2 male | . ... ... | ... | ... | ... | ... | ... | . 84477 dict2/立て-377_10_1_female.mp3 | 頭高 | たて | 2 | 1 | dict2 female | . 84478 dict2/立てる-377_11_1_male.mp3 | 中高 | たてる | 3 | 2 | dict2 male | . 84479 dict2/立てる-377_11_1_female.mp3 | 中高 | たてる | 3 | 2 | dict2 female | . 84480 dict2/立とう-377_12_1_male.mp3 | 中高 | たとう | 3 | 2 | dict2 male | . 84481 dict2/立とう-377_12_1_female.mp3 | 中高 | たとう | 3 | 2 | dict2 female | . 84470 rows × 6 columns . The first dictionary is what I started with. It has recordings by 3-4 male voices, which word is pronounced by which speaker is not marked anywhere. I had to parse a json file to get the labels for it. . I was afraid that my model wouldn&#39;t generalize well, so I needed the data from the second dictionary. The most important was having at least one female voice in the training data, and having a male voice not present in the training data for the validation set was nice as well. The pitch accent labels for these recordings were xml inside a json file. . Feautre extraction . Let&#39;s convert our previous example audio into a tensor and then make a spectrogram out of it. . at = AudioTensor.create(&quot;あめー雨.mp3&quot;) at.show() . Your browser does not support the audio element. &lt;AxesSubplot:&gt; . To make a spectrogram in fastaudio the easiest way is to first create a config object with parameters: . cfg = AudioConfig.Voice() . aud2spec = AudioToSpec.from_cfg(cfg) show_image(aud2spec(at)) . &lt;AxesSubplot:&gt; . I don&#39;t know why it comes out flipped, but it&#39;s easy to fix . show_image(aud2spec(at)).invert_yaxis() . Let&#39;s load a couple of examples to try this configuration on them . ex_paths = [ Path(&#39;dict2&#39;).ls()[20006], Path(&#39;dict2&#39;).ls()[20007],Path(&#39;dict1&#39;).ls()[21345]] ex_paths . [Path(&#39;dict2/取り残した-473_4_1_male.mp3&#39;), Path(&#39;dict2/取り残して-473_3_1_female.mp3&#39;), Path(&#39;dict1/不人気.yomi0003F319_0108.mp3&#39;)] . Let&#39;s make a function to plot a spectrogram for path with config. . def show_spectro_cfg(cfg, path): at = AudioTensor.create(path) aud2spec = AudioToSpec.from_cfg(cfg) spec = aud2spec(at) show_image(spec, figsize=(12,8)).invert_yaxis() return spec.shape . for path in ex_paths: print(show_spectro_cfg(cfg, path)) . torch.Size([1, 128, 397]) torch.Size([1, 128, 460]) torch.Size([1, 128, 532]) . These are good enough and I got to 98% accuracy with them, but let&#39;s see if we can do better. . To see what the parameters for AudioConfig mean let&#39;s look at torchaudio docs. . Edit: this looks like a much better resource. . ??torchaudio.transforms.MelSpectrogram . Init signature: torchaudio.transforms.MelSpectrogram( sample_rate: int = 16000, n_fft: int = 400, win_length: Optional[int] = None, hop_length: Optional[int] = None, f_min: float = 0.0, f_max: Optional[float] = None, pad: int = 0, n_mels: int = 128, window_fn: Callable[..., torch.Tensor] = &lt;built-in method hann_window of type object at 0x7f2703e74d00&gt;, power: Optional[float] = 2.0, normalized: bool = False, wkwargs: Optional[dict] = None, center: bool = True, pad_mode: str = &#39;reflect&#39;, onesided: bool = True, norm: Optional[str] = None, ) -&gt; None Source: class MelSpectrogram(torch.nn.Module): r&#34;&#34;&#34;Create MelSpectrogram for a raw audio signal. This is a composition of Spectrogram and MelScale. Sources * https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe * https://timsainb.github.io/spectrograms-mfccs-and-inversion-in-python.html * http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html Args: sample_rate (int, optional): Sample rate of audio signal. (Default: ``16000``) win_length (int or None, optional): Window size. (Default: ``n_fft``) hop_length (int or None, optional): Length of hop between STFT windows. (Default: ``win_length // 2``) n_fft (int, optional): Size of FFT, creates ``n_fft // 2 + 1`` bins. (Default: ``400``) f_min (float, optional): Minimum frequency. (Default: ``0.``) f_max (float or None, optional): Maximum frequency. (Default: ``None``) pad (int, optional): Two sided padding of signal. (Default: ``0``) n_mels (int, optional): Number of mel filterbanks. (Default: ``128``) window_fn (Callable[..., Tensor], optional): A function to create a window tensor that is applied/multiplied to each frame/window. (Default: ``torch.hann_window``) wkwargs (Dict[..., ...] or None, optional): Arguments for window function. (Default: ``None``) center (bool, optional): whether to pad :attr:`waveform` on both sides so that the :math:`t`-th frame is centered at time :math:`t times text{hop _length}`. Default: ``True`` pad_mode (string, optional): controls the padding method used when :attr:`center` is ``True``. Default: ``&#34;reflect&#34;`` onesided (bool, optional): controls whether to return half of results to avoid redundancy. Default: ``True`` norm (Optional[str]): If &#39;slaney&#39;, divide the triangular mel weights by the width of the mel band (area normalization). (Default: ``None``) Example &gt;&gt;&gt; waveform, sample_rate = torchaudio.load(&#39;test.wav&#39;, normalization=True) &gt;&gt;&gt; mel_specgram = transforms.MelSpectrogram(sample_rate)(waveform) # (channel, n_mels, time) &#34;&#34;&#34; __constants__ = [&#39;sample_rate&#39;, &#39;n_fft&#39;, &#39;win_length&#39;, &#39;hop_length&#39;, &#39;pad&#39;, &#39;n_mels&#39;, &#39;f_min&#39;] def __init__(self, sample_rate: int = 16000, n_fft: int = 400, win_length: Optional[int] = None, hop_length: Optional[int] = None, f_min: float = 0., f_max: Optional[float] = None, pad: int = 0, n_mels: int = 128, window_fn: Callable[..., Tensor] = torch.hann_window, power: Optional[float] = 2., normalized: bool = False, wkwargs: Optional[dict] = None, center: bool = True, pad_mode: str = &#34;reflect&#34;, onesided: bool = True, norm: Optional[str] = None) -&gt; None: super(MelSpectrogram, self).__init__() self.sample_rate = sample_rate self.n_fft = n_fft self.win_length = win_length if win_length is not None else n_fft self.hop_length = hop_length if hop_length is not None else self.win_length // 2 self.pad = pad self.power = power self.normalized = normalized self.n_mels = n_mels # number of mel frequency bins self.f_max = f_max self.f_min = f_min self.spectrogram = Spectrogram(n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length, pad=self.pad, window_fn=window_fn, power=self.power, normalized=self.normalized, wkwargs=wkwargs, center=center, pad_mode=pad_mode, onesided=onesided) self.mel_scale = MelScale(self.n_mels, self.sample_rate, self.f_min, self.f_max, self.n_fft // 2 + 1, norm) def forward(self, waveform: Tensor) -&gt; Tensor: r&#34;&#34;&#34; Args: waveform (Tensor): Tensor of audio of dimension (..., time). Returns: Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time). &#34;&#34;&#34; specgram = self.spectrogram(waveform) mel_specgram = self.mel_scale(specgram) return mel_specgram File: ~/.conda/envs/default/lib/python3.9/site-packages/torchaudio/transforms.py Type: type Subclasses: . I decided to decrease the upper bound on frequency, since we don&#39;t really need the information contained in there for pitch and increase the window size, so as to get better low frequency resolution at the cost of temporal resolution. The rest is heuristics. . pitch_cfg = AudioConfig.Voice(f_min=0, f_max=1200, n_fft=1024*4, win_length=1024*2) . for path in ex_paths: show_spectro_cfg(pitch_cfg, path) . I would like to also add minimum decibel cut-off like you can in torchaudio.transforms.AmplitudeToDB to add contrast and remove noise, but I wasn&#39;t able to find that option if it exists in fastaudio. . Preparing our labels for training . Let&#39;s throw out anything that&#39;s not atamadaka, nakadaka or heiban for now. . dict1 = dict1[dict1.pattern.isin([&#39;頭高&#39;, &#39;中高&#39;, &#39;平板&#39;])] . dict2 = dict2[dict2.pattern.isin([&#39;頭高&#39;, &#39;中高&#39;, &#39;平板&#39;])] . Add the whole path. . dict1.path = &#39;dict1/&#39;+dict1.path . dict2.path = &#39;dict2/&#39;+dict2.path . Merge the labels into one file. . all_labels = pd.concat([dict1, dict2]).reset_index(drop=True) . Convert the labels into romaji, so we can plot the later. . all_labels.pattern.replace([&#39;頭高&#39;, &#39;中高&#39;, &#39;平板&#39;], [&#39;atamadaka&#39;, &#39;nakadaka&#39;, &#39;heiban&#39;], inplace=True) . Add a column, that we&#39;re going to use to split the data into training and validation set. . all_labels[&#39;is_valid&#39;] = False . all_labels.loc[all_labels.type == &#39;dict2 male&#39;, &#39;is_valid&#39;] = True . all_labels . path pattern kana morae drop type is_valid . 0 dict1/ある.yomi000142BB_0596.mp3 | atamadaka | アル | 2 | 1 | dict1 | False | . 1 dict1/思う.yomi0006C617_043A.mp3 | nakadaka | オモウ | 3 | 2 | dict1 | False | . 2 dict1/など.yomi000240B7_0028.mp3 | atamadaka | ナド | 2 | 1 | dict1 | False | . 3 dict1/私.yomi00092F63_0072.mp3 | heiban | ワタくシ | 4 | 0 | dict1 | False | . 4 dict1/見る.yomi000A41BD_001E.mp3 | atamadaka | ミル | 2 | 1 | dict1 | False | . ... ... | ... | ... | ... | ... | ... | ... | . 160758 dict2/立て-377_10_1_female.mp3 | atamadaka | たて | 2 | 1 | dict2 female | False | . 160759 dict2/立てる-377_11_1_male.mp3 | nakadaka | たてる | 3 | 2 | dict2 male | True | . 160760 dict2/立てる-377_11_1_female.mp3 | nakadaka | たてる | 3 | 2 | dict2 female | False | . 160761 dict2/立とう-377_12_1_male.mp3 | nakadaka | たとう | 3 | 2 | dict2 male | True | . 160762 dict2/立とう-377_12_1_female.mp3 | nakadaka | たとう | 3 | 2 | dict2 female | False | . 160763 rows × 7 columns . Let&#39;s take a sample of the data to make quick experiments. . sample_df = all_labels.sample(16000) . Making a Learner and training . I&#39;m closely following Zach Mueller&#39;s lesson on audio here. . Let&#39;s remove the silence from our files (this is going to be most useful at inference), make our data the same size and convert them to spectrograms using the usual configuration. . item_tfms = [RemoveSilence, ResizeSignal(2000), aud2spec] . Preparing the Dataloaders . def get_x(df): return df.path def get_y(df): return df.pattern . dblock = DataBlock(blocks=[AudioBlock, CategoryBlock], item_tfms=item_tfms, get_x=get_x, get_y=get_y, splitter=ColSplitter()) . dls = dblock.dataloaders(sample_df, shuffle=True) . AudioSpectrogram.show() throws out an error for me so I had to hardcode around it by making this function. . def show_spec_batch(dls): _,axes = plt.subplots(3,3, figsize=(12,8)) for (spec, pattern),ax in zip(dls.show_batch(show=False)[2], axes.flatten()): show_image(spec, title=pattern, ax=ax).invert_yaxis() . show_spec_batch(dls) . Adding n_out=3 for the number of classes we have. . learn = Learner(dls, xresnet34(pretrained=True, n_out=3), CrossEntropyLossFlat(), metrics=[accuracy, F1Score(average=&#39;weighted&#39;)], wd=0.05).to_fp16() . Our spectrograms only have one channel, so we have to change the first Conv Layer. . def alter_learner(learn): layer = learn.model[0][0] layer.in_channels = 1 layer.weight = nn.Parameter(layer.weight[:,1,:,:].unsqueeze(1)) learn.model[0][0] = layer . alter_learner(learn) . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=0.33113112449646, lr_steep=2.7542285919189453) . learn.fit_one_cycle(7, 3e-2) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.271414 | 3.523385 | 0.625543 | 0.499470 | 00:55 | . 1 | 0.196330 | 3.277715 | 0.445437 | 0.402725 | 00:52 | . 2 | 0.155347 | 0.152431 | 0.947610 | 0.945649 | 00:52 | . 3 | 0.111935 | 0.165354 | 0.934814 | 0.935485 | 00:52 | . 4 | 0.071950 | 0.171538 | 0.937711 | 0.934709 | 00:52 | . 5 | 0.051120 | 0.083185 | 0.972718 | 0.972574 | 00:52 | . 6 | 0.045892 | 0.065998 | 0.976098 | 0.976093 | 00:52 | . learn.recorder.plot_loss() . Let&#39;s make that a function for easier experimenting. . def make_pitch_learner(df, item_tfms, model=xresnet34(pretrained=True, n_out=3)): new_dblock = DataBlock(blocks=[AudioBlock, CategoryBlock], item_tfms=item_tfms, get_x=get_x, get_y=get_y, splitter=ColSplitter()) new_dls = new_dblock.dataloaders(df, shuffle=True) new_learn = Learner(new_dls, model, CrossEntropyLossFlat(), metrics=[accuracy, F1Score(average=&#39;weighted&#39;)], wd=0.05).to_fp16() alter_learner(new_learn) new_learn.unfreeze() return new_learn . Let&#39;s try it with the spectrogram parameters we chose. . learn2 = make_pitch_learner(sample_df, [RemoveSilence(), ResizeSignal(2000, AudioPadType.Zeros), AudioToSpec.from_cfg(pitch_cfg)]) . learn2.lr_find() . SuggestedLRs(lr_min=0.19054607152938843, lr_steep=1.5848932266235352) . learn2.fit_one_cycle(7, 3e-2) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.329706 | 2.352126 | 0.608402 | 0.462043 | 01:22 | . 1 | 0.206030 | 0.551953 | 0.828102 | 0.829986 | 01:23 | . 2 | 0.154101 | 0.194723 | 0.952921 | 0.952699 | 01:22 | . 3 | 0.119368 | 0.105970 | 0.963061 | 0.963882 | 01:22 | . 4 | 0.091843 | 0.103478 | 0.964510 | 0.964183 | 01:22 | . 5 | 0.063340 | 0.068436 | 0.975616 | 0.976229 | 01:22 | . 6 | 0.046640 | 0.060200 | 0.981169 | 0.981281 | 01:22 | . learn2.recorder.plot_loss() . It&#39;d be nice to use torchaudio.transforms.PitchShift here, which would force my model to generalize better, but it seems the torchaudio version supported doesn&#39;t have that yet. . I tried to implement it on my own, but to no avail. . torchaudio.sox_effects.effect_names() . [&#39;allpass&#39;, &#39;band&#39;, &#39;bandpass&#39;, &#39;bandreject&#39;, &#39;bass&#39;, &#39;bend&#39;, &#39;biquad&#39;, &#39;chorus&#39;, &#39;channels&#39;, &#39;compand&#39;, &#39;contrast&#39;, &#39;dcshift&#39;, &#39;deemph&#39;, &#39;delay&#39;, &#39;dither&#39;, &#39;divide&#39;, &#39;downsample&#39;, &#39;earwax&#39;, &#39;echo&#39;, &#39;echos&#39;, &#39;equalizer&#39;, &#39;fade&#39;, &#39;fir&#39;, &#39;firfit&#39;, &#39;flanger&#39;, &#39;gain&#39;, &#39;highpass&#39;, &#39;hilbert&#39;, &#39;loudness&#39;, &#39;lowpass&#39;, &#39;mcompand&#39;, &#39;norm&#39;, &#39;oops&#39;, &#39;overdrive&#39;, &#39;pad&#39;, &#39;phaser&#39;, &#39;pitch&#39;, &#39;rate&#39;, &#39;remix&#39;, &#39;repeat&#39;, &#39;reverb&#39;, &#39;reverse&#39;, &#39;riaa&#39;, &#39;silence&#39;, &#39;sinc&#39;, &#39;speed&#39;, &#39;stat&#39;, &#39;stats&#39;, &#39;stretch&#39;, &#39;swap&#39;, &#39;synth&#39;, &#39;tempo&#39;, &#39;treble&#39;, &#39;tremolo&#39;, &#39;trim&#39;, &#39;upsample&#39;, &#39;vad&#39;, &#39;vol&#39;] . class SoxEffectTransform(torch.nn.Module): def __init__(self, effects): super().__init__() self.effects = effects self.rate = 16000 def forward(self, tensor: torch.Tensor): return torchaudio.sox_effects.apply_effects_tensor( tensor, self.rate, self.effects) . effects = [ [&#39;pitch&#39;, &#39;10&#39;] ] . PitchShift = SoxEffectTransform(effects) . at_shift = PitchShift(at)[0] . torchaudio.sox_effects.init_sox_effects() . Audio(at_shift, rate=24000) . Your browser does not support the audio element. Final model . So, let&#39;s finalize our project by training a bigger model on the whole dataset. . fin_learn = make_pitch_learner(all_labels, [RemoveSilence(), ResizeSignal(2000, AudioPadType.Zeros), AudioToSpec.from_cfg(pitch_cfg)], xresnet50(pretrained=True, n_out=3)) . show_spec_batch(fin_learn.dls) . fin_learn.lr_find() . SuggestedLRs(lr_min=0.0006309573538601399, lr_steep=0.3019951581954956) . fin_learn.fit_one_cycle(4, 3e-4) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.091439 | 0.182869 | 0.930946 | 0.935078 | 15:07 | . 1 | 0.050077 | 0.076877 | 0.974457 | 0.974723 | 14:16 | . 2 | 0.033197 | 0.076989 | 0.977416 | 0.977832 | 13:47 | . 3 | 0.024043 | 0.070777 | 0.979973 | 0.980235 | 13:36 | . fin_learn.recorder.plot_loss() . fin_learn.export(&quot;removesilence_pitch.pkl&quot;) . fin_learn.lr_find(end_lr=3e-3) . SuggestedLRs(lr_min=3.8196694163161736e-08, lr_steep=8.417093340540305e-06) . fin_learn.fit_one_cycle(1, 3e-7) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.023801 | 0.068112 | 0.981677 | 0.981916 | 13:16 | . fin_learn.export(&quot;removesilence_pitch2.pkl&quot;) . fin_learn.fit_one_cycle(1, slice(1e-7, 5e-5)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.022423 | 0.078075 | 0.978623 | 0.978896 | 13:06 | . fin_learn.fit_one_cycle(1, slice(1e-7, 5e-3)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.038483 | 0.059628 | 0.982719 | 0.982791 | 13:55 | . fin_learn.export(&#39;removesilence_pitch3.pkl&#39;) . I&#39;m getting pretty desperate here, because I had gotten an accuracy of 0.984 before with the default spectrogram parameters. . fin_learn.fit_one_cycle(1, slice(1e-7, 5e-4)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.029828 | 0.060020 | 0.983476 | 0.983565 | 13:41 | . fin_learn.fit_one_cycle(1, slice(1e-8, 1e-4)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.019335 | 0.059957 | 0.982932 | 0.983027 | 13:36 | . fin_learn.fit_one_cycle(1, slice(1e-8, 1e-4)) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.026071 | 0.065069 | 0.981417 | 0.981602 | 13:33 | . fin_learn.fit_one_cycle(1, slice(1e-6, 1e-4)) . Was my previous model maybe learning some patterns from the phonemes as well? . Inference . inf_learn = load_learner(&quot;removesilence_pitch3.pkl&quot;) . Confirming that the saved learner works. . spec,pred,predtens,probs = inf_learn.predict(&#39;あめー雨.mp3&#39;, with_input=True) show_image(spec) print(probs, pred) . tensor([0.6475, 0.3349, 0.0176]) atamadaka . spec,pred,predtens,probs = inf_learn.predict(&#39;あめー雨.mp3&#39;, with_input=True) show_image(spec) print(probs, pred) . tensor([0.3057, 0.6840, 0.0103]) heiban . This model is not robust, I ended up not using it. . HuggingFace Space . I used HuggingFace spaces with Gradio to deploy the model. . I created a new space. Then I added a requirements.txt file with the following python libraries . fastaudio librosa soundfile . Then after some googling I made a packages.txt with the name of a C package libsndfile1, that we need for fastaudio to work properly, in it. . Then you create an app.py file. . Importing dependencies. . import gradio as gr from fastai.vision.all import * from fastaudio.core.all import * . Loading the learner. . def get_x(df): return df.path def get_y(df): return df.pattern learn = load_learner(&#39;xresnet50_pitch3.pkl&#39;) labels = learn.dls.vocab . We need a function that will return the predictions and a spectrogram from the inputs it gets. We want users to be able to both upload and record audio. . def predict(Record, Upload): if Upload: path = Upload else: path = Record spec,pred,pred_idx,probs = learn.predict(str(path), with_input=True) fig,ax = plt.subplots(figsize=(16,10)) show_image(spec, ax=ax) ax.invert_yaxis() return [{labels[i]: float(probs[i]) for i in range(len(labels))}, fig] . So as not to make the spectrogram ourselves once again we pass with_input=True to the predict method. Gradio&#39;s gr.outputs.Image can take in a figure from matplotlib, so we create that first. . We return a list with a dictionary for probabilities for the labels and a spectrogram figure. . Preparing other parameters. . title = &quot;Japanese Pitch Accent Pattern Detector&quot; description = &quot;This model will predict the pitch accent pattern of a word based on the recording of its pronunciation.&quot; article=&quot;&lt;p style=&#39;text-align: center&#39;&gt;&lt;a href=&#39;https://mizoru.github.io/blog&#39; target=&#39;_blank&#39;&gt;Blog&lt;/a&gt;&lt;/p&gt;&quot; examples = [[&#39;代わる.mp3&#39;],[&#39;大丈夫な.mp3&#39;],[&#39;熱くない.mp3&#39;], [&#39;あめー雨.mp3&#39;], [&#39;あめー飴.mp3&#39;]] enable_queue=True . Putting everything into this final call. . gr.Interface(fn=predict,inputs=[gr.inputs.Audio(source=&#39;microphone&#39;, type=&#39;filepath&#39;, optional=True), gr.inputs.Audio(source=&#39;upload&#39;, type=&#39;filepath&#39;, optional=True)], outputs= [gr.outputs.Label(num_top_classes=3), gr.outputs.Image(type=&quot;plot&quot;, label=&#39;Spectrogram&#39;)], title=title, description=description, article=article, examples=examples).launch(debug=True, enable_queue=enable_queue) .",
            "url": "https://mizoru.github.io/blog/2021/12/25/Japanese-pitch.html",
            "relUrl": "/2021/12/25/Japanese-pitch.html",
            "date": " • Dec 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Classifying pitch accent from spectrograms with XResNet",
            "content": "import fastaudio from fastaudio.core.all import * from fastai.vision.all import * . /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/torchaudio/backend/utils.py:46: UserWarning: &#34;torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE&#34; flag is deprecated and will be removed in 0.9.0. Please remove the use of flag. warnings.warn( . %config InlineBackend.figure_format = &#39;retina&#39; . Path() . Path(&#39;.&#39;) . . . path = Path()/&#39;cleanAudio&#39; . . (#1) [Path(&#39;labels_no_duplicates.csv&#39;)] . . data = pd.read_csv(&#39;labels_no_duplicates.csv&#39;, low_memory=False) . data . Unnamed: 0 path pattern kana syl drop . 0 0 | ある.yomi000142BB_0596.mp3 | 頭高 | アル | 2 | 1 | . 1 2 | 思う.yomi0006C617_043A.mp3 | 中高 | オモウ | 3 | 2 | . 2 3 | など.yomi000240B7_0028.mp3 | 頭高 | ナド | 2 | 1 | . 3 4 | 私.yomi00092F63_0072.mp3 | 平板 | ワタくシ | 4 | 0 | . 4 5 | 見る.yomi000A41BD_001E.mp3 | 頭高 | ミル | 2 | 1 | . ... ... | ... | ... | ... | ... | ... | . 66489 74183 | 捨てがな_捨て仮名.yomi00072538_06BE.mp3 | 平板 | すテカ゚ナ | 5 | 0 | . 66490 74185 | くも膜下出血_蜘蛛膜下出血.yomi0001AAD1_0622.mp3 | 中高 | クモマッカしュッケツ | 10 | 6 | . 66491 74187 | 捜す.yomi00072507_0088.mp3 | 平板 | サカ゚ス | 4 | 0 | . 66492 74188 | 捜し物.yomi000724FD_0424.mp3 | 平板 | サカ゚シモノ | 6 | 0 | . 66493 74189 | あこや貝_阿古屋貝.yomi00013767_0114.mp3 | 中高 | アコヤカ゚イ | 6 | 3 | . 66494 rows × 6 columns . cnfg = AudioConfig.Voice() . at = AudioTensor.create(path/&#39;捨てがな_捨て仮名.yomi00072538_06BE.mp3&#39;) . aud2spec = AudioToSpec.from_cfg(cnfg) . aud2spec(at).show() . TypeError Traceback (most recent call last) /tmp/ipykernel_71/2151244571.py in &lt;module&gt; -&gt; 1 aud2spec(at).show() ~/.conda/envs/default/lib/python3.9/site-packages/fastaudio/core/spectrogram.py in show(self, ctx, ax, title, **kwargs) 75 def show(self, ctx=None, ax=None, title=&#34;&#34;, **kwargs): 76 &#34;Show spectrogram using librosa&#34; &gt; 77 return show_spectrogram(self, ctx=ctx, ax=ax, title=title, **kwargs) 78 79 ~/.conda/envs/default/lib/python3.9/site-packages/fastaudio/core/spectrogram.py in show_spectrogram(sg, title, ax, ctx, **kwargs) 86 # x_start, y_start, x_lenght, y_lenght, all in percent 87 ia = ax.inset_axes((i / sg.nchannels, 0.2, 1 / sg.nchannels, 0.7)) &gt; 88 z = specshow( 89 channel.cpu().numpy(), ax=ia, **sg._all_show_args(show_y=i == 0), **kwargs 90 ) ~/.conda/envs/default/lib/python3.9/site-packages/librosa/display.py in specshow(data, x_coords, y_coords, x_axis, y_axis, sr, hop_length, fmin, fmax, tuning, bins_per_octave, key, Sa, mela, thaat, ax, **kwargs) 854 # Set up axis scaling 855 __scale_axes(axes, x_axis, &#34;x&#34;) --&gt; 856 __scale_axes(axes, y_axis, &#34;y&#34;) 857 858 # Construct tickers and locators ~/.conda/envs/default/lib/python3.9/site-packages/librosa/display.py in __scale_axes(axes, ax_type, which) 972 return 973 --&gt; 974 scaler(mode, **kwargs) 975 976 ~/.conda/envs/default/lib/python3.9/site-packages/matplotlib/axes/_base.py in set_yscale(self, value, **kwargs) 4098 g = self.get_shared_y_axes() 4099 for ax in g.get_siblings(self): -&gt; 4100 ax.yaxis._set_scale(value, **kwargs) 4101 ax._update_transScale() 4102 ax.stale = True ~/.conda/envs/default/lib/python3.9/site-packages/matplotlib/axis.py in _set_scale(self, value, **kwargs) 759 def _set_scale(self, value, **kwargs): 760 if not isinstance(value, mscale.ScaleBase): --&gt; 761 self._scale = mscale.scale_factory(value, self, **kwargs) 762 else: 763 self._scale = value ~/.conda/envs/default/lib/python3.9/site-packages/matplotlib/scale.py in scale_factory(scale, axis, **kwargs) 595 scale = scale.lower() 596 scale_cls = _api.check_getitem(_scale_mapping, scale=scale) --&gt; 597 return scale_cls(axis, **kwargs) 598 599 TypeError: __init__() got an unexpected keyword argument &#39;linthreshy&#39; . at.show() . crop2s = ResizeSignal(2000) . data = data.iloc[:,1:].set_index(&#39;path&#39;) . data . pattern kana syl drop . path . ある.yomi000142BB_0596.mp3 頭高 | アル | 2 | 1 | . 思う.yomi0006C617_043A.mp3 中高 | オモウ | 3 | 2 | . など.yomi000240B7_0028.mp3 頭高 | ナド | 2 | 1 | . 私.yomi00092F63_0072.mp3 平板 | ワタくシ | 4 | 0 | . 見る.yomi000A41BD_001E.mp3 頭高 | ミル | 2 | 1 | . ... ... | ... | ... | ... | . 捨てがな_捨て仮名.yomi00072538_06BE.mp3 平板 | すテカ゚ナ | 5 | 0 | . くも膜下出血_蜘蛛膜下出血.yomi0001AAD1_0622.mp3 中高 | クモマッカしュッケツ | 10 | 6 | . 捜す.yomi00072507_0088.mp3 平板 | サカ゚ス | 4 | 0 | . 捜し物.yomi000724FD_0424.mp3 平板 | サカ゚シモノ | 6 | 0 | . あこや貝_阿古屋貝.yomi00013767_0114.mp3 中高 | アコヤカ゚イ | 6 | 3 | . 66494 rows × 4 columns . data.loc[&#39;ある.yomi000142BB_0596.mp3&#39;][0] . &#39;頭高&#39; . def get_label(fname): return data.loc[fname][0] . dblock = DataBlock(blocks=[AudioBlock, CategoryBlock], get_items=get_audio_files, item_tfms=[crop2s, aud2spec], get_y=using_attr(get_label, &#39;name&#39;), splitter=RandomSubsetSplitter(0.1, 0.02) ) . dls = dblock.dataloaders(path, bs=32, shuffle=True) . len(dls.get_idxs()) . dls.one_batch()[0].shape . learn = Learner(dls, xresnet50(pretrained=True), metrics=[accuracy, F1Score(average=&#39;weighted&#39;)]) . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . learn = learn.to_fp16() . learn.model[0][0].in_channels def alter_learner(learn): layer = learn.model[0][0] layer.in_channels = 1 layer.weight = nn.Parameter(layer.weight[:,1,:,:].unsqueeze(1)) learn.model[0][0] = layer . alter_learner(learn) . learn.fine_tune(5, 3e-3) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.424598 | 4.219926 | 0.439428 | 0.277089 | 00:34 | . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.205065 | 0.845803 | 0.760722 | 0.740047 | 00:33 | . 1 | 0.203014 | 0.758320 | 0.784048 | 0.767731 | 00:33 | . 2 | 0.143726 | 0.127169 | 0.960873 | 0.956737 | 00:33 | . 3 | 0.111206 | 0.120738 | 0.963130 | 0.956488 | 00:34 | . 4 | 0.076544 | 0.093413 | 0.968397 | 0.964488 | 00:34 | . learn.lr_find() . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=9.12010818865383e-07) . learn.fit(3, slice(3e-7, 3e-6), wd=0.05) . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.078476 | 0.092806 | 0.966892 | 0.962685 | 00:33 | . 1 | 0.080976 | 0.093654 | 0.967645 | 0.963156 | 00:33 | . 2 | 0.083820 | 0.091037 | 0.968397 | 0.964490 | 00:33 | . Balanced Dataset . import random random.seed(42) nakadaka = random.sample(data[data.pattern == &#39;中高&#39;].index.values.tolist(), 2400) atamadaka = random.sample(data[data.pattern == &#39;頭高&#39;].index.values.tolist(), 2400) heiban = random.sample(data[data.pattern == &#39;平板&#39;].index.values.tolist(), 2400) . ds = [(i, &#39;nakadaka&#39;) for i in nakadaka] + [(i, &#39;atamadaka&#39;) for i in atamadaka] + [(i, &#39;heiban&#39;) for i in heiban] df = pd.DataFrame(ds) . df[0] = path / df[0] . def get_x(df): return df[0] def get_y(df): return df[1] . dblock = DataBlock(blocks=[AudioBlock, CategoryBlock], get_x=get_x, item_tfms=[crop2s, aud2spec], get_y=get_y, splitter=RandomSplitter(valid_pct=0.2) ) . dls2 = dblock.dataloaders(df) . learn2 = Learner(dls2, xresnet50(pretrained=True), CrossEntropyLossFlat(), metrics=[accuracy, F1Score(average=&#39;weighted&#39;)], wd=0.05).to_fp16() . alter_learner(learn2) . learn2.lr_find() . SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.015848932787775993) . learn2.fine_tune(7, 9e-3) . epoch train_loss valid_loss accuracy f1_score time . 0 | 1.734659 | 0.703426 | 0.713889 | 0.700657 | 00:26 | . epoch train_loss valid_loss accuracy f1_score time . 0 | 0.363018 | 0.286336 | 0.909028 | 0.908712 | 00:26 | . 1 | 0.229606 | 0.172058 | 0.947917 | 0.948306 | 00:26 | . 2 | 0.133098 | 0.254065 | 0.905556 | 0.902066 | 00:26 | . 3 | 0.088526 | 0.105291 | 0.961111 | 0.961012 | 00:26 | . 4 | 0.060891 | 0.073923 | 0.967361 | 0.967326 | 00:27 | . 5 | 0.043505 | 0.065347 | 0.975694 | 0.975679 | 00:27 | . 6 | 0.033957 | 0.058501 | 0.975694 | 0.975704 | 00:27 | . learn2.recorder.plot_loss() . Data Augmentation . from fastaudio.augment.all import * . DBMelSpec = SpectrogramTransformer() . aud2spec.settings . {&#39;mel&#39;: &#39;True&#39;, &#39;to_db&#39;: &#39;False&#39;, &#39;sample_rate&#39;: 16000, &#39;n_fft&#39;: 1024, &#39;win_length&#39;: 1024, &#39;hop_length&#39;: 128, &#39;f_min&#39;: 50.0, &#39;f_max&#39;: 8000.0, &#39;pad&#39;: 0, &#39;n_mels&#39;: 128, &#39;window_fn&#39;: &lt;function _VariableFunctionsClass.hann_window&gt;, &#39;power&#39;: 2.0, &#39;normalized&#39;: False, &#39;wkwargs&#39;: None, &#39;center&#39;: True, &#39;pad_mode&#39;: &#39;reflect&#39;, &#39;onesided&#39;: True, &#39;norm&#39;: None, &#39;stype&#39;: &#39;power&#39;, &#39;top_db&#39;: None} . . aud2spec(at).show() . TypeError Traceback (most recent call last) /tmp/ipykernel_71/2151244571.py in &lt;module&gt; -&gt; 1 aud2spec(at).show() ~/.conda/envs/default/lib/python3.9/site-packages/fastaudio/core/spectrogram.py in show(self, ctx, ax, title, **kwargs) 75 def show(self, ctx=None, ax=None, title=&#34;&#34;, **kwargs): 76 &#34;Show spectrogram using librosa&#34; &gt; 77 return show_spectrogram(self, ctx=ctx, ax=ax, title=title, **kwargs) 78 79 ~/.conda/envs/default/lib/python3.9/site-packages/fastaudio/core/spectrogram.py in show_spectrogram(sg, title, ax, ctx, **kwargs) 86 # x_start, y_start, x_lenght, y_lenght, all in percent 87 ia = ax.inset_axes((i / sg.nchannels, 0.2, 1 / sg.nchannels, 0.7)) &gt; 88 z = specshow( 89 channel.cpu().numpy(), ax=ia, **sg._all_show_args(show_y=i == 0), **kwargs 90 ) ~/.conda/envs/default/lib/python3.9/site-packages/librosa/display.py in specshow(data, x_coords, y_coords, x_axis, y_axis, sr, hop_length, fmin, fmax, tuning, bins_per_octave, key, Sa, mela, thaat, ax, **kwargs) 854 # Set up axis scaling 855 __scale_axes(axes, x_axis, &#34;x&#34;) --&gt; 856 __scale_axes(axes, y_axis, &#34;y&#34;) 857 858 # Construct tickers and locators ~/.conda/envs/default/lib/python3.9/site-packages/librosa/display.py in __scale_axes(axes, ax_type, which) 972 return 973 --&gt; 974 scaler(mode, **kwargs) 975 976 ~/.conda/envs/default/lib/python3.9/site-packages/matplotlib/axes/_base.py in set_yscale(self, value, **kwargs) 4098 g = self.get_shared_y_axes() 4099 for ax in g.get_siblings(self): -&gt; 4100 ax.yaxis._set_scale(value, **kwargs) 4101 ax._update_transScale() 4102 ax.stale = True ~/.conda/envs/default/lib/python3.9/site-packages/matplotlib/axis.py in _set_scale(self, value, **kwargs) 759 def _set_scale(self, value, **kwargs): 760 if not isinstance(value, mscale.ScaleBase): --&gt; 761 self._scale = mscale.scale_factory(value, self, **kwargs) 762 else: 763 self._scale = value ~/.conda/envs/default/lib/python3.9/site-packages/matplotlib/scale.py in scale_factory(scale, axis, **kwargs) 595 scale = scale.lower() 596 scale_cls = _api.check_getitem(_scale_mapping, scale=scale) --&gt; 597 return scale_cls(axis, **kwargs) 598 599 TypeError: __init__() got an unexpected keyword argument &#39;linthreshy&#39; . item_tfms=[RemoveSilence(), ResizeSignal(2000), aud2spec, MaskTime(1,size=4), MaskFreq(1,size=3)] . show_image(Pipeline(item_tfms)(at)) . &lt;AxesSubplot:&gt; . show_image(Pipeline(item_tfms)(at)) . &lt;AxesSubplot:&gt; . dblock = DataBlock(blocks=[AudioBlock, CategoryBlock], get_x=get_x, item_tfms=item_tfms, get_y=get_y, splitter=RandomSplitter(valid_pct=0.2) ) . dls3 = dblock.dataloaders(df, ) . learn3 = Learner(dls3, xresnet50(True), CrossEntropyLossFlat(), wd=0.05, metrics=[accuracy, F1Score(average=&#39;weighted&#39;)]).to_fp16() . alter_learner(learn3) . learn3.lr_find() . . 0.00% [0/2 00:00&lt;00:00] . 37.78% [34/90 00:09&lt;00:16 7.2763] learn3.fine_tune(10, 7e-3) .",
            "url": "https://mizoru.github.io/blog/2020/12/11/pitch_cnn.html",
            "relUrl": "/2020/12/11/pitch_cnn.html",
            "date": " • Dec 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Исследование студентов НИУ ВШЭ",
            "content": "&#1053;&#1072;&#1078;&#1084;&#1080;&#1090;&#1077; &#1085;&#1072; &#1082;&#1085;&#1086;&#1087;&#1082;&#1091;, &#1095;&#1090;&#1086;&#1073;&#1099; &#1087;&#1088;&#1080;&#1089;&#1090;&#1091;&#1087;&#1080;&#1090;&#1100; &#1082; &#1101;&#1082;&#1089;&#1087;&#1077;&#1088;&#1080;&#1084;&#1077;&#1085;&#1090;&#1091;: .",
            "url": "https://mizoru.github.io/blog/2020/12/08/experiment.html",
            "relUrl": "/2020/12/08/experiment.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "import pandas as pd . . This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. . Below are the versions of fastai, fastcore, wwf, fastaudio, and torchaudio currently running at the time of writing this: . fastai : 2.1.5 | fastcore : 1.3.4 | wwf : 0.0.16 | fastaudio : 0.1.3 | torchaudio : 0.9.0 | . . import pkg_resources def placeholder(x): raise pkg_resources.DistributionNotFound pkg_resources.get_distribution = placeholder import fastai import fastaudio from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * . data = pd.read_csv(&quot;comma labels.csv&quot;, index_col=&#39;path&#39;) . data . pattern kana syl drop . path . ある.yomi000142BB_0596.mp3 頭高 | アル | 2 | 1 | . ある.yomi000142BB_0596.mp3 頭高 | アル | 2 | 1 | . 思う.yomi0006C617_043A.mp3 中高 | オモウ | 3 | 2 | . など.yomi000240B7_0028.mp3 頭高 | ナド | 2 | 1 | . 私.yomi00092F63_0072.mp3 平板 | ワタくシ | 4 | 0 | . ... ... | ... | ... | ... | . くも膜下出血_蜘蛛膜下出血.yomi0001AAD1_0622.mp3 中高 | クモマッカしュッケツ | 10 | 6 | . 捜す.yomi00072507_0088.mp3 平板 | サカ゚ス | 4 | 0 | . 捜し物.yomi000724FD_0424.mp3 平板 | サカ゚シモノ | 6 | 0 | . あこや貝_阿古屋貝.yomi00013767_0114.mp3 中高 | アコヤカ゚イ | 6 | 3 | . あこや貝_阿古屋貝.yomi00013767_0114.mp3 中高 | アコヤカ゚イ | 6 | 3 | . 74191 rows × 4 columns . data.loc[&#39;１すくい_一掬い.yomi000120BB_01C8.mp3&#39;] . pattern kana syl drop . path . １すくい_一掬い.yomi000120BB_01C8.mp3 中高 | ひトすクイ | 5 | 2 | . １すくい_一掬い.yomi000120BB_01C8.mp3 中高 | ひトすクイ | 5 | 2 | . data.loc[&#39;出し抜く.yomi0004BDF8_06F2.mp3&#39;] . pattern 平板 kana ダシヌク syl 4 drop 0 Name: 出し抜く.yomi0004BDF8_06F2.mp3, dtype: object . data.loc[&#39;ある.yomi000142BB_0596.mp3&#39;] . pattern kana syl drop . path . ある.yomi000142BB_0596.mp3 頭高 | アル | 2 | 1 | . ある.yomi000142BB_0596.mp3 頭高 | アル | 2 | 1 | . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . path = Path(&#39;/content/drive/MyDrive&#39;) . path = path / &#39;1000sample&#39; . at = AudioTensor.create(path.ls()[0]) . at.show() . Your browser does not support the audio element. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7a5b38aed0&gt; . cfg = AudioConfig.Voice() . aud2spec = AudioToSpec.from_cfg(cfg) . aud2spec(at).show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7a5b50f950&gt; . crop2s = ResizeSignal(2000) . crop2s(at).show() . Your browser does not support the audio element. &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7a5b50f5d0&gt; . pipe = Pipeline([AudioTensor.create, crop2s, aud2spec]) . for i in range(5): pipe(path.ls()[i]).show() . . data.loc[&#39;ある.yomi000142BB_0596.mp3&#39;][&#39;pattern&#39;] . &#39;頭高&#39; . data.reset_index()[data.reset_index().duplicated(keep=False)].to_csv(&#39;duplicates.csv&#39;) . data[data.index.duplicated(keep=False)].to_csv(&#39;path_dup.csv&#39;) . data.duplicated().sum() . 15695 . data.reset_index() . path pattern kana syl drop . 0 ある.yomi000142BB_0596.mp3 | 頭高 | アル | 2 | 1 | . 1 ある.yomi000142BB_0596.mp3 | 頭高 | アル | 2 | 1 | . 2 思う.yomi0006C617_043A.mp3 | 中高 | オモウ | 3 | 2 | . 3 など.yomi000240B7_0028.mp3 | 頭高 | ナド | 2 | 1 | . 4 私.yomi00092F63_0072.mp3 | 平板 | ワタくシ | 4 | 0 | . ... ... | ... | ... | ... | ... | . 74186 くも膜下出血_蜘蛛膜下出血.yomi0001AAD1_0622.mp3 | 中高 | クモマッカしュッケツ | 10 | 6 | . 74187 捜す.yomi00072507_0088.mp3 | 平板 | サカ゚ス | 4 | 0 | . 74188 捜し物.yomi000724FD_0424.mp3 | 平板 | サカ゚シモノ | 6 | 0 | . 74189 あこや貝_阿古屋貝.yomi00013767_0114.mp3 | 中高 | アコヤカ゚イ | 6 | 3 | . 74190 あこや貝_阿古屋貝.yomi00013767_0114.mp3 | 中高 | アコヤカ゚イ | 6 | 3 | . 74191 rows × 5 columns . data = data.reset_index() . removing complete duplicates . data = data[-data.duplicated()] . data.to_csv(&#39;labels complete duplicates removed.csv&#39;) . duplicate labels . data.duplicated(subset=[&#39;path&#39;], keep=False).sum() . 687 . data = data[-data.duplicated(subset=[&#39;path&#39;], keep=False)] . data.to_csv(&#39;labels_no_duplicates.csv&#39;) . sr = data[data.path == &#39;捜す.yomi00072507_0088.mp3&#39;][&#39;pattern&#39;].values[0] . sr.values[0] . &#39;平板&#39; . item_tfms = [ResizeSignal(1000), aud2spec] . def get_label(fn): return data[data.path == fn][&#39;pattern&#39;].values[0] . audpitch = DataBlock(blocks=(AudioBlock, CategoryBlock), get_items=get_audio_files, splitter=RandomSplitter(), item_tfms = [crop2s, aud2spec], get_y=using_attr(get_label, &#39;name&#39;)) . for fn in path.ls(): print(fn) try: get_label(fn.name) except: fn.unlink() . for fn in path.ls(): print(fn) print(get_label(fn.name)) . learn = Learner(dls, xresnet18(), CrossEntropyLossFlat(), metrics=accuracy) . n_c = dls.one_batch()[0].shape[1]; n_c . 1 . def alter_learner(learn, n_channels=1): &quot;Adjust a `Learner`&#39;s model to accept `1` channel&quot; layer = learn.model[0][0] layer.in_channels=n_channels layer.weight = nn.Parameter(layer.weight[:,1,:,:].unsqueeze(1)) learn.model[0][0] = layer . alter_learner(learn, n_c) . learn.lr_find() . /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . SuggestedLRs(lr_min=0.03019951581954956, lr_steep=0.00363078061491251) . learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.250854 | 0.228218 | 0.937931 | 00:12 | . 1 | 0.246093 | 1.324811 | 0.544828 | 00:12 | . 2 | 0.241305 | 0.378529 | 0.882759 | 00:12 | . 3 | 0.232387 | 0.211727 | 0.951724 | 00:12 | . 4 | 0.225047 | 0.219429 | 0.965517 | 00:12 | . learn.fit_one_cycle(5, 1e-4) . epoch train_loss valid_loss accuracy time . 0 | 0.204818 | 0.208217 | 0.958621 | 00:12 | . 1 | 0.204138 | 0.206059 | 0.951724 | 00:12 | . 2 | 0.202360 | 0.217017 | 0.958621 | 00:12 | . 3 | 0.198590 | 0.214045 | 0.958621 | 00:12 | . 4 | 0.198357 | 0.220390 | 0.944828 | 00:12 | . train = dls.train . m = learn.model . y = dls[0] . names = [fn.name for fn in path.ls()] . datasample = data[data.path.isin(names)] . datasample.groupby(&#39;pattern&#39;).count() . path kana syl drop . pattern . 中高 229 | 229 | 229 | 229 | . 尾高 18 | 18 | 18 | 18 | . 平板 403 | 403 | 403 | 403 | . 頭高 78 | 78 | 78 | 78 | . data.groupby(&#39;pattern&#39;).count() . path kana syl drop . pattern . 中高 24444 | 24444 | 24444 | 24444 | . 尾高 1461 | 1461 | 1461 | 1461 | . 平板 30551 | 30551 | 30551 | 30551 | . 頭高 10038 | 10038 | 10038 | 10038 | .",
            "url": "https://mizoru.github.io/blog/2020/11/07/resnet-pitch-accent.html",
            "relUrl": "/2020/11/07/resnet-pitch-accent.html",
            "date": " • Nov 7, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mizoru.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mizoru.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mizoru.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mizoru.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}